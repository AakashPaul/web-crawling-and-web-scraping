{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "##general\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# Each website is a separate project (folder)\n",
    "def create_project_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        print('Creating directory ' + directory)\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "# Create queue and crawled files (if not created)\n",
    "def create_data_files(project_name, base_url):\n",
    "    queue = os.path.join(project_name , 'queue.txt')\n",
    "    crawled = os.path.join(project_name,\"crawled.txt\")\n",
    "    if not os.path.isfile(queue):\n",
    "        write_file(queue, base_url)\n",
    "    if not os.path.isfile(crawled):\n",
    "        write_file(crawled, '')\n",
    "\n",
    "\n",
    "# Create a new file\n",
    "def write_file(path, data):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(data)\n",
    "\n",
    "\n",
    "# Add data onto an existing file\n",
    "def append_to_file(path, data):\n",
    "    with open(path, 'a') as file:\n",
    "        file.write(data + '\\n')\n",
    "\n",
    "\n",
    "# Delete the contents of a file\n",
    "def delete_file_contents(path):\n",
    "    open(path, 'w').close()\n",
    "\n",
    "\n",
    "# Read a file and convert each line to set items\n",
    "def file_to_set(file_name):\n",
    "    results = set()\n",
    "    with open(file_name, 'rt') as f:\n",
    "        for line in f:\n",
    "            results.add(line.replace('\\n', ''))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Iterate through a set, each item will be a line in a file\n",
    "def set_to_file(links, file_name):\n",
    "    with open(file_name,\"w\") as f:\n",
    "        for l in sorted(links):\n",
    "            f.write(l+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "##link finder\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "from urllib import parse\n",
    "\n",
    "class LinkFinder(HTMLParser):\n",
    "    def __init__(self,base_url,page_url):\n",
    "        super().__init__()\n",
    "        self.base_url=base_url\n",
    "        self.page_url=page_url\n",
    "        self.links=set()\n",
    "        \n",
    "    def handle_starttag(self,tag,attrs):\n",
    "        if tag=='a':\n",
    "            for (attribute,value) in attrs:\n",
    "                if attribute == 'href':\n",
    "                    url=parse.urljoin(self.base_url,value)\n",
    "                    self.links.add(url)\n",
    "    \n",
    "    def page_links(self):\n",
    "        return self.links\n",
    "    \n",
    "    def error(self,message):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "##spider\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "class Spider:\n",
    "\n",
    "    project_name = ''\n",
    "    base_url = ''\n",
    "    domain_name = ''\n",
    "    queue_file = ''\n",
    "    crawled_file = ''\n",
    "    queue = set()\n",
    "    crawled = set()\n",
    "\n",
    "    def __init__(self, project_name, base_url, domain_name):\n",
    "        Spider.project_name = project_name\n",
    "        Spider.base_url = base_url\n",
    "        Spider.domain_name = domain_name\n",
    "        Spider.queue_file = Spider.project_name + '/queue.txt'\n",
    "        Spider.crawled_file = Spider.project_name + '/crawled.txt'\n",
    "        self.boot()\n",
    "        self.crawl_page('First spider', Spider.base_url)\n",
    "\n",
    "    # Creates directory and files for project on first run and starts the spider\n",
    "    @staticmethod\n",
    "    def boot():\n",
    "        create_project_dir(Spider.project_name)\n",
    "        create_data_files(Spider.project_name, Spider.base_url)\n",
    "        Spider.queue = file_to_set(Spider.queue_file)\n",
    "        Spider.crawled = file_to_set(Spider.crawled_file)\n",
    "\n",
    "    # Updates user display, fills queue and updates files\n",
    "    @staticmethod\n",
    "    def crawl_page(thread_name, page_url):\n",
    "        if page_url not in Spider.crawled:\n",
    "            print(thread_name + ' now crawling ' + page_url)\n",
    "            print('Queue ' + str(len(Spider.queue)) + ' | Crawled  ' + str(len(Spider.crawled)))\n",
    "            Spider.add_links_to_queue(Spider.gather_links(page_url))\n",
    "            Spider.queue.remove(page_url)\n",
    "            Spider.crawled.add(page_url)\n",
    "            Spider.update_files()\n",
    "\n",
    "    # Converts raw response data into readable information and checks for proper html formatting\n",
    "    @staticmethod\n",
    "    def gather_links(page_url):\n",
    "        html_string = ''\n",
    "        try:\n",
    "            response = urlopen(page_url)\n",
    "            if 'text/html' in response.getheader('Content-Type'):\n",
    "                html_bytes = response.read()\n",
    "                html_string = html_bytes.decode(\"utf-8\")\n",
    "            finder = LinkFinder(Spider.base_url, page_url)\n",
    "            finder.feed(html_string)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            return set()\n",
    "        return finder.page_links()\n",
    "\n",
    "    # Saves queue data to project files\n",
    "    @staticmethod\n",
    "    def add_links_to_queue(links):\n",
    "        for url in links:\n",
    "            if (url in Spider.queue) or (url in Spider.crawled):\n",
    "                continue\n",
    "            if Spider.domain_name != get_domain_name(url):\n",
    "                continue\n",
    "            Spider.queue.add(url)\n",
    "\n",
    "    @staticmethod\n",
    "    def update_files():\n",
    "        set_to_file(Spider.queue, Spider.queue_file)\n",
    "        set_to_file(Spider.crawled, Spider.crawled_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "##domain\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "# Get domain name (example.com)\n",
    "def get_domain_name(url):\n",
    "    try:\n",
    "        results = get_sub_domain_name(url).split('.')\n",
    "        return results[-2] + '.' + results[-1]\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "\n",
    "# Get sub domain name (name.example.com)\n",
    "def get_sub_domain_name(url):\n",
    "    try:\n",
    "        return urlparse(url).netloc\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory blackcoffer\n",
      "First spider now crawling https://www.blackcoffer.com/\n",
      "Queue 1 | Crawled  0\n",
      "27 links in the queue\n",
      "Thread-14 now crawling https://www.blackcoffer.com/staff.htmlThread-21 now crawling https://www.blackcoffer.com/about.html\n",
      "Thread-16 now crawling https://www.blackcoffer.com/industries.htmlQueue 27 | Crawled  1Thread-17 now crawling https://www.blackcoffer.com/index.html\n",
      "\n",
      "Thread-20 now crawling https://qna.blackcoffer.com/Queue 27 | Crawled  1\n",
      "Queue 27 | Crawled  1\n",
      "Thread-18 now crawling https://www.blackcoffer.com/information.html\n",
      "Queue 27 | Crawled  1\n",
      "Thread-15 now crawling https://www.blackcoffer.com/research.html\n",
      "Queue 27 | Crawled  1\n",
      "Queue 27 | Crawled  1\n",
      "\n",
      "\n",
      "Thread-19 now crawling https://www.blackcoffer.com/healthcare.html\n",
      "\n",
      "Queue 27 | Crawled  1Queue 27 | Crawled  1\n",
      "\n",
      "HTTP Error 403: Forbidden\n",
      "Thread-20 now crawling https://insights.blackcoffer.com/\n",
      "Queue 26 | Crawled  3\n",
      "Thread-21 now crawling https://www.blackcoffer.com/bigdata.html\n",
      "Queue 26 | Crawled  3\n",
      "Thread-14 now crawling https://www.blackcoffer.com/careers.html\n",
      "Queue 26 | Crawled  9\n",
      "Thread-15 now crawling https://jobs.blackcoffer.com/Thread-19 now crawling http://dataintel.blackcoffer.comThread-18 now crawling https://www.blackcoffer.com/application.html\n",
      "Queue 26 | Crawled  9\n",
      "Thread-17 now crawling http://freelance.blackcoffer.com\n",
      "Queue 26 | Crawled  9\n",
      "\n",
      "Queue 26 | Crawled  9\n",
      "Thread-16 now crawling https://www.blackcoffer.com/contact.html\n",
      "\n",
      "Queue 26 | Crawled  9\n",
      "Queue 26 | Crawled  9\n",
      "HTTP Error 403: Forbidden\n",
      "Thread-20 now crawling https://www.blackcoffer.com/products.html\n",
      "Queue 25 | Crawled  10\n",
      "Thread-21 now crawling https://www.blackcoffer.com/consulting.html\n",
      "Queue 24 | Crawled  11\n",
      "Thread-14 now crawling https://www.blackcoffer.com/supply.html\n",
      "Queue 23 | Crawled  12\n",
      "Thread-19 now crawling http://dataoil.blackcoffer.com\n",
      "Queue 33 | Crawled  13\n",
      "Thread-18 now crawling https://www.blackcoffer.com/energy.html\n",
      "Queue 31 | Crawled  15\n",
      "Thread-16 now crawling https://www.blackcoffer.com/services.html\n",
      "Queue 31 | Crawled  15\n",
      "Thread-20 now crawling https://www.blackcoffer.com/bce.html\n",
      "Queue 31 | Crawled  16\n",
      "Thread-21 now crawling https://www.blackcoffer.com/fmcg.html\n",
      "Queue 30 | Crawled  17\n",
      "Thread-14 now crawling https://www.blackcoffer.com/portals.html\n",
      "Queue 30 | Crawled  18\n",
      "Thread-16 now crawling https://www.blackcoffer.com/bfsi.htmlThread-18 now crawling https://www.blackcoffer.com/aboutbc.html\n",
      "Queue 28 | Crawled  20\n",
      "\n",
      "Queue 28 | Crawled  20\n",
      "138 links in the queue\n",
      "Thread-19 now crawling http://blog.blackcoffer.comThread-21 now crawling https://jobs.blackcoffer.com/register/Thread-20 now crawling https://jobs.blackcoffer.com/type/temporary/Thread-14 now crawling https://freelance.blackcoffer.com/job/5-need-competitve-coders/Thread-16 now crawling https://freelance.blackcoffer.com/job-category/design-creative/Thread-18 now crawling https://freelance.blackcoffer.com/job-category/web-mobile-software-development/Thread-17 now crawling https://jobs.blackcoffer.com/job/?keyword=data+analystThread-15 now crawling https://www.blackcoffer.com/request_demo.php\n",
      "Queue 138 | Crawled  28\n",
      "\n",
      "Queue 138 | Crawled  28\n",
      "\n",
      "Queue 138 | Crawled  28\n",
      "\n",
      "\n",
      "Queue 138 | Crawled  28\n",
      "\n",
      "Queue 138 | Crawled  28\n",
      "\n",
      "Queue 138 | Crawled  28\n",
      "\n",
      "Queue 138 | Crawled  28Queue 138 | Crawled  28\n",
      "\n",
      "Thread-15 now crawling https://freelance.blackcoffer.com/job/5-virtual-assistant-required-for-ongoing-work/\n",
      "Queue 137 | Crawled  29\n",
      "HTTP Error 403: Forbidden\n",
      "Thread-19 now crawling https://freelance.blackcoffer.com/my-bookmarks/\n",
      "Queue 136 | Crawled  30\n",
      "Thread-14 now crawling https://www.blackcoffer.com/CaseStudies/Capital/Blackcoffer - Anti-Fraud Analytics.pdf\n",
      "Queue 141 | Crawled  31\n",
      "Thread-18 now crawling https://www.blackcoffer.com/sample.htmlThread-16 now crawling http://www.blackcoffer.com/index.html\n",
      "Queue 152 | Crawled  34\n",
      "\n",
      "Queue 152 | Crawled  34\n",
      "Thread-21 now crawling https://freelance.blackcoffer.com/my-account/lost-password/\n",
      "Queue 152 | Crawled  34\n",
      "Thread-19 now crawling https://freelance.blackcoffer.com/job-category/web-development/\n",
      "Queue 151 | Crawled  35\n",
      "HTTP Error 400: Bad Request\n",
      "Thread-14 now crawling https://jobs.blackcoffer.com/job-categories/\n",
      "Queue 151 | Crawled  37\n",
      "Thread-15 now crawling https://jobs.blackcoffer.com/job/angular-developer/\n",
      "Queue 151 | Crawled  37\n",
      "HTTP Error 404: Not Found\n",
      "Thread-18 now crawling https://freelance.blackcoffer.com/job/5-tutor-for-django-jquery-problem-paid-by-hour/\n",
      "Queue 150 | Crawled  38\n",
      "Thread-20 now crawling https://freelance.blackcoffer.com/job-category/machine-learning/\n",
      "Queue 151 | Crawled  39\n",
      "Thread-16 now crawling https://freelance.blackcoffer.com/about-us/\n",
      "Queue 150 | Crawled  41\n",
      "Thread-21 now crawling https://freelance.blackcoffer.com/Freelancer%20Membership%20Agreement\n",
      "Queue 150 | Crawled  41\n",
      "Thread-17 now crawling https://freelance.blackcoffer.com/submit-profile/\n",
      "Queue 156 | Crawled  42\n",
      "Thread-19 now crawling https://freelance.blackcoffer.com/job/5-find-me-3-canvas-print-factories-for-me-in-the-huadu-area-of-guangdong-china/\n",
      "Queue 161 | Crawled  43\n",
      "Thread-18 now crawling https://freelance.blackcoffer.com/job/5-need-a-recommender-system-for-my-video-app/\n",
      "Queue 164 | Crawled  44\n",
      "Thread-14 now crawling https://freelance.blackcoffer.com/job/5-orange-pi-pc-and-armbian-i2s-codec-driver/\n",
      "Queue 264 | Crawled  45\n",
      "Thread-20 now crawling https://freelance.blackcoffer.com/job-alerts/\n",
      "Queue 276 | Crawled  47\n",
      "Thread-15 now crawling https://blackcoffer.com/industries.html\n",
      "Queue 276 | Crawled  47\n",
      "Thread-16 now crawling https://jobs.blackcoffer.com/type/full-time/\n",
      "Queue 275 | Crawled  48\n",
      "Thread-19 now crawling https://freelance.blackcoffer.com/most-rating/\n",
      "Queue 275 | Crawled  49\n",
      "Thread-17 now crawling https://jobs.blackcoffer.com/job/02-front-end-developers/?preview=true\n",
      "Queue 274 | Crawled  50\n",
      "Thread-18 now crawling https://www.blackcoffer.com/CaseStudies/Retail/Blackcoffer - Customer Analytics.pdf\n",
      "Queue 272 | Crawled  52\n",
      "Thread-15 now crawling https://jobs.blackcoffer.com/job/digital-designer/?preview=true\n",
      "Queue 272 | Crawled  52\n",
      "Thread-14 now crawling https://jobs.blackcoffer.com/type/part-time/\n",
      "Queue 273 | Crawled  53\n",
      "Thread-20 now crawling https://jobs.blackcoffer.com/job/senior-process-analyst/?preview=true\n",
      "Queue 277 | Crawled  54\n",
      "HTTP Error 400: Bad Request\n",
      "Thread-18 now crawling https://jobs.blackcoffer.com/login/\n",
      "Queue 276 | Crawled  55\n",
      "Thread-21 now crawling https://freelance.blackcoffer.com/browse-project/\n",
      "Queue 275 | Crawled  56\n",
      "Thread-19 now crawling https://freelance.blackcoffer.com/job/5-build-simple-laravel-project/\n",
      "Queue 274 | Crawled  57\n",
      "Thread-16 now crawling https://www.blackcoffer.com/PORTALS.html\n",
      "Queue 291 | Crawled  58\n",
      "Thread-18 now crawling https://freelance.blackcoffer.com/job/5-setup-digital-ocean-for-multiple-site-with-security-packages/\n",
      "Queue 290 | Crawled  59\n",
      "HTTP Error 404: Not Found\n",
      "Thread-16 now crawling https://jobs.blackcoffer.com/request-talent/\n",
      "Queue 289 | Crawled  60\n",
      "Thread-19 now crawling https://www.blackcoffer.com/#signup-dialog\n",
      "Queue 290 | Crawled  61\n",
      "Thread-17 now crawling https://freelance.blackcoffer.com/post-a-project/\n",
      "Queue 290 | Crawled  63\n",
      "Thread-15 now crawling https://dataoil.blackcoffer.com/index.html\n",
      "Queue 290 | Crawled  63\n",
      "Thread-18 now crawling https://jobs.blackcoffer.com/job/ios-developer/\n",
      "Queue 290 | Crawled  64\n",
      "Thread-19 now crawling http://Dataintel.blackcoffer.com\n",
      "Queue 289 | Crawled  65\n",
      "Thread-16 now crawling https://freelance.blackcoffer.com/browse-project-categories\n",
      "Queue 290 | Crawled  68\n",
      "Thread-21 now crawling https://insights.blackcoffer.com/m\n",
      "Thread-15 now crawling https://jobs.blackcoffer.com/faqs/Queue 290 | Crawled  68\n",
      "\n",
      "Queue 290 | Crawled  68\n",
      "Thread-19 now crawling https://jobs.blackcoffer.com/job/android-developer/\n",
      "Queue 288 | Crawled  70\n",
      "HTTP Error 403: Forbidden\n",
      "Thread-17 now crawling https://www.blackcoffer.com/CaseStudies/Finance/Blackcoffer - Anti-Fraud Analytics.pdf\n",
      "Queue 287 | Crawled  71\n",
      "Thread-21 now crawling https://jobs.blackcoffer.com/job/?keyword=designer\n",
      "Queue 287 | Crawled  71\n",
      "Thread-14 now crawling https://freelance.blackcoffer.com/job/5-complete-laravel-website-which-has-several-issues/\n",
      "Queue 290 | Crawled  72\n",
      "HTTP Error 400: Bad Request\n",
      "Thread-17 now crawling https://jobs.blackcoffer.com/candidates/\n",
      "Queue 289 | Crawled  73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-15 now crawling https://freelance.blackcoffer.com/job/5-mailchimp-template/\n",
      "Queue 288 | Crawled  74\n",
      "Thread-19 now crawling https://jobs.blackcoffer.com/term-and-conditions/\n",
      "Queue 287 | Crawled  75\n",
      "Thread-14 now crawling https://jobs.blackcoffer.com/job/?keyword=data+science\n",
      "Thread-16 now crawling https://blackcoffer.com/request_demo.php\n",
      "Queue 383 | Crawled  77Queue 383 | Crawled  77\n",
      "\n",
      "Thread-15 now crawling https://jobs.blackcoffer.com/contact-us/\n",
      "Queue 383 | Crawled  78\n",
      "Thread-21 now crawling https://jobs.blackcoffer.com/job/uxui-designer/\n",
      "Queue 387 | Crawled  79\n",
      "Thread-16 now crawling https://freelance.blackcoffer.com/job/5-full-stack-development-react-js-node-js/\n",
      "Queue 386 | Crawled  80\n",
      "Thread-19 now crawling https://jobs.blackcoffer.com/lost-password/\n",
      "Queue 385 | Crawled  81\n",
      "Thread-17 now crawling https://jobs.blackcoffer.com/job/?keyword=linux\n",
      "Queue 405 | Crawled  82\n",
      "Thread-15 now crawling https://jobs.blackcoffer.com/job/front-end-engineer/\n",
      "Queue 404 | Crawled  83\n",
      "Thread-16 now crawling https://www.blackcoffer.com/CaseStudies/Retail/Blackcoffer - Stories of eCommerce and Payment System Firms.pdf\n",
      "Queue 403 | Crawled  84\n",
      "Thread-19 now crawling https://jobs.blackcoffer.com/job/?keyword=marketing\n",
      "Queue 402 | Crawled  85\n",
      "Thread-14 now crawling https://freelance.blackcoffer.com/resumes/\n",
      "Queue 401 | Crawled  86\n",
      "HTTP Error 400: Bad Request\n",
      "Thread-16 now crawling https://freelance.blackcoffer.com/Privacy%20Policy\n",
      "Queue 400 | Crawled  87\n",
      "Thread-17 now crawling https://jobs.blackcoffer.com/job/react-developer/\n",
      "Queue 400 | Crawled  88\n",
      "Thread-14 now crawling https://jobs.blackcoffer.com/dashboard/\n",
      "Queue 409 | Crawled  89\n",
      "Thread-19 now crawling https://freelance.blackcoffer.com/blog/\n",
      "Queue 411 | Crawled  90\n",
      "Thread-16 now crawling https://www.blackcoffer.com/CaseStudies/Government/Blackcoffer - Think Tanks.pdf\n",
      "Queue 410 | Crawled  91\n",
      "HTTP Error 400: Bad Request\n",
      "Thread-16 now crawling https://www.blackcoffer.com/#login-dialog\n",
      "Queue 409 | Crawled  92\n",
      "Thread-19 now crawling https://www.blackcoffer.com/CaseStudies/Healthcare/Blackcoffer - Healthcare Insurance Analytics.pdf\n",
      "Queue 498 | Crawled  93\n",
      "Thread-16 now crawling https://www.blackcoffer.com/CaseStudies/Pharma/Blackcoffer - Customer Analytics.pdfThread-17 now crawling https://jobs.blackcoffer.com/job/javascript-developer/\n",
      "Queue 496 | Crawled  95\n",
      "\n",
      "Queue 496 | Crawled  95\n",
      "HTTP Error 400: Bad Request\n",
      "Thread-19 now crawling https://jobs.blackcoffer.com/job/?keyword=ios+developer\n",
      "Queue 495 | Crawled  96\n",
      "HTTP Error 400: Bad Request\n",
      "Thread-16 now crawling https://dataintel.blackcoffer.com/\n",
      "Queue 494 | Crawled  97\n",
      "Thread-15 now crawling https://jobs.blackcoffer.com/job/\n",
      "Queue 493 | Crawled  98\n",
      "Thread-16 now crawling https://freelance.blackcoffer.com/job/5-computer-graphics-jobs/\n",
      "Queue 492 | Crawled  99\n",
      "Thread-19 now crawling https://freelance.blackcoffer.com/job/5-product-designer-for-router-access-point-wifi-extender-and-associated-software-os/\n",
      "Queue 492 | Crawled  100\n",
      "Thread-19 now crawling https://jobs.blackcoffer.com/dashboard/?iwj_tab=profile\n",
      "Queue 492 | Crawled  103\n",
      "Thread-16 now crawling https://freelance.blackcoffer.com/job/5-i-need-a-graphic-designer-for-an-explainer-video-animation/\n",
      "Queue 492 | Crawled  103\n",
      "Thread-15 now crawling https://jobs.blackcoffer.com/job/?keyword=senior\n",
      "Queue 491 | Crawled  104\n",
      "Thread-18 now crawling https://www.blackcoffer.com/CaseStudies/Supplychain/Blackcoffer - Supply Chain Analytics.pdf\n",
      "Queue 491 | Crawled  104\n",
      "HTTP Error 400: Bad Request\n",
      "Thread-18 now crawling https://freelance.blackcoffer.com/job/5-changes-on-an-existing-website-drupal-7/\n",
      "Queue 490 | Crawled  105\n",
      "Thread-20 now crawling https://freelance.blackcoffer.com/job/5-seo-web-desing-site-about-art-volunteer/\n",
      "Queue 489 | Crawled  107\n",
      "Thread-21 now crawling https://jobs.blackcoffer.com/job/software-development-java/\n",
      "Queue 489 | Crawled  107\n",
      "Thread-16 now crawling https://jobs.blackcoffer.com/job/student-marketing-manager-fashion-lifestyle-brand/\n",
      "Queue 488 | Crawled  108\n",
      "Thread-18 now crawling https://freelance.blackcoffer.com/freelancer-dashboard/\n",
      "Queue 487 | Crawled  109\n",
      "Thread-21 now crawling https://freelance.blackcoffer.com/job-category/data-entry/\n",
      "Queue 486 | Crawled  110\n",
      "Thread-20 now crawling https://www.blackcoffer.com/social.html\n",
      "Queue 485 | Crawled  111\n",
      "Thread-21 now crawling https://www.blackcoffer.com/dip.html\n",
      "Queue 484 | Crawled  112\n",
      "Thread-19 now crawling https://jobs.blackcoffer.com/employers/\n",
      "Queue 483 | Crawled  113\n",
      "Thread-14 now crawling https://freelance.blackcoffer.com/job/5-amazon-store/\n",
      "Queue 482 | Crawled  114\n",
      "HTTP Error 404: Not Found\n",
      "HTTP Error 404: Not Found\n",
      "Thread-21 now crawling http://www.blackcoffer.com/products.html\n",
      "Queue 480 | Crawled  116\n",
      "Thread-20 now crawling https://freelance.blackcoffer.com/User%20Agreement\n",
      "Queue 480 | Crawled  116\n",
      "Thread-21 now crawling https://jobs.blackcoffer.com\n",
      "Queue 479 | Crawled  117\n",
      "Thread-21 now crawling https://jobs.blackcoffer.com/job/3d-modeler/\n",
      "Queue 478 | Crawled  118\n",
      "Thread-20 now crawling https://jobs.blackcoffer.com/about-us-3/\n",
      "Queue 477 | Crawled  119\n",
      "Thread-17 now crawling https://www.blackcoffer.com/data.html\n",
      "Queue 477 | Crawled  120\n",
      "HTTP Error 404: Not Found\n",
      "Thread-20 now crawling https://freelance.blackcoffer.com/job/5-restyling-web-applications-logo/\n",
      "Queue 476 | Crawled  121\n",
      "HTTP Error 404: Not Found\n",
      "Thread-17 now crawling https://jobs.blackcoffer.com/job/php-web-software-developer-zend-mvc-html-css-javascript/\n",
      "Queue 475 | Crawled  122\n",
      "Thread-20 now crawling https://www.blackcoffer.com/fact.html\n",
      "Queue 474 | Crawled  123\n",
      "Thread-21 now crawling https://freelance.blackcoffer.com/job/5-design-me-a-coffee-shop/\n",
      "Queue 473 | Crawled  124\n",
      "HTTP Error 404: Not Found\n",
      "Thread-20 now crawling https://jobs.blackcoffer.com/job/analyst/\n",
      "Queue 472 | Crawled  125\n",
      "Thread-18 now crawling https://jobs.blackcoffer.com/job/?keyword=Intern\n",
      "Queue 471 | Crawled  126\n",
      "Thread-17 now crawling https://www.blackcoffer.com/CaseStudies/Capital/Blackcoffer Fraud Analytics.pdf\n",
      "Queue 470 | Crawled  127\n",
      "Thread-21 now crawling https://blackcoffer.com\n",
      "Queue 469 | Crawled  128\n",
      "HTTP Error 400: Bad Request\n",
      "Thread-17 now crawling https://jobs.blackcoffer.com/job/?keyword=android\n",
      "Queue 468 | Crawled  129\n",
      "Thread-16 now crawling https://jobs.blackcoffer.com/privacy-policy/\n",
      "Queue 468 | Crawled  130\n",
      "Thread-21 now crawling https://freelance.blackcoffer.com/job/5-crowd-counting/\n",
      "Queue 467 | Crawled  132\n",
      "Thread-20 now crawling https://freelance.blackcoffer.com/job/5-webshop-like-amazon/\n",
      "Queue 467 | Crawled  132\n",
      "Thread-18 now crawling https://jobs.blackcoffer.com/about-us/\n",
      "Queue 468 | Crawled  133\n",
      "Thread-20 now crawling https://freelance.blackcoffer.com/job/5-game-developing-in-createjs/\n",
      "Queue 467 | Crawled  134\n",
      "Thread-21 now crawling https://www.blackcoffer.com/text.html\n",
      "Queue 466 | Crawled  135\n",
      "Thread-16 now crawling https://jobs.blackcoffer.com/job/data-scientist-quant/\n",
      "Queue 465 | Crawled  136\n",
      "HTTP Error 404: Not Found\n",
      "Thread-21 now crawling https://freelance.blackcoffer.com/job/5-product-uploading-jobs-basic-only-copy-paste-fixed-30/\n",
      "Queue 464 | Crawled  137\n",
      "Thread-17 now crawling https://freelance.blackcoffer.com/job-category/artificial-intelligence/Thread-15 now crawling https://www.blackcoffer.com/CaseStudies/Insurance/Blackcoffer - Healthcare Insurance Analytics.pdf\n",
      "Queue 465 | Crawled  140\n",
      "\n",
      "Queue 464 | Crawled  141\n",
      "Thread-20 now crawling https://www.blackcoffer.com/CaseStudies/Research/Blackcoffer - Academics.pdf\n",
      "Queue 464 | Crawled  141\n",
      "Thread-18 now crawling https://jobs.blackcoffer.com/job/wordpress-developer/\n",
      "Queue 464 | Crawled  141\n",
      "HTTP Error 400: Bad Request\n",
      "Thread-15 now crawling https://jobs.blackcoffer.com/job/maths-teacher-second-in-department-ks3-ks4/?preview=true\n",
      "Queue 463 | Crawled  142\n",
      "HTTP Error 400: Bad Request\n",
      "Thread-20 now crawling https://freelance.blackcoffer.com/job-category/data-science-analytics/\n",
      "Queue 462 | Crawled  143\n",
      "Thread-19 now crawling https://freelance.blackcoffer.com/job-category/graphic-design/\n",
      "Queue 473 | Crawled  145\n",
      "Thread-21 now crawling https://jobs.blackcoffer.com/how-it-work/\n",
      "Queue 473 | Crawled  145\n",
      "Thread-16 now crawling https://blackcoffer.com/services.html\n",
      "Queue 472 | Crawled  146\n",
      "Thread-17 now crawling https://jobs.blackcoffer.com/dashboard/?iwj_tab=new-job\n",
      "Queue 471 | Crawled  147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-14 now crawling https://freelance.blackcoffer.com/Terms%20of%20Use\n",
      "Queue 470 | Crawled  148\n",
      "Thread-16 now crawling https://freelance.blackcoffer.com/employers-dashboard/\n",
      "Queue 469 | Crawled  149\n",
      "Thread-18 now crawling https://freelance.blackcoffer.com/how-it-works/\n",
      "Queue 469 | Crawled  150\n",
      "Thread-20 now crawling https://www.blackcoffer.com/CaseStudies/Finance/Blackcoffer Fraud Analytics.pdf\n",
      "Queue 470 | Crawled  152\n",
      "Thread-19 now crawling https://freelance.blackcoffer.com/job/5-sql-coding/\n",
      "Queue 470 | Crawled  152\n",
      "HTTP Error 400: Bad Request\n",
      "Thread-20 now crawling https://freelance.blackcoffer.com/contact/\n",
      "Queue 469 | Crawled  153\n",
      "Thread-16 now crawling https://blackcoffer.com/products.html\n",
      "Queue 468 | Crawled  154\n",
      "Thread-19 now crawling https://freelance.blackcoffer.com/browse-project-categories/\n",
      "Queue 467 | Crawled  155\n",
      "Thread-17 now crawling https://jobs.blackcoffer.com/job/?keyword=developerThread-18 now crawling https://freelance.blackcoffer.com/Thread-20 now crawling https://jobs.blackcoffer.com/job-suggestion/\n",
      "Queue 463 | Crawled  159\n",
      "\n",
      "\n",
      "Queue 463 | Crawled  159\n",
      "Queue 463 | Crawled  159\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-9e0a32723d80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mcreate_workers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-96-9e0a32723d80>\u001b[0m in \u001b[0;36mcrawl\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueued_links\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueued_links\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' links in the queue'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mcreate_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-9e0a32723d80>\u001b[0m in \u001b[0;36mcreate_jobs\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-9e0a32723d80>\u001b[0m in \u001b[0;36mcrawl\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueued_links\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueued_links\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' links in the queue'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mcreate_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-9e0a32723d80>\u001b[0m in \u001b[0;36mcreate_jobs\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_to_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQUEUE_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_tasks_done\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munfinished_tasks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_tasks_done\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##main\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "PROJECT_NAME = 'blackcoffer'\n",
    "base_url = 'https://www.blackcoffer.com/'\n",
    "DOMAIN_NAME = get_domain_name(base_url)\n",
    "QUEUE_FILE = PROJECT_NAME + '/queue.txt'\n",
    "CRAWLED_FILE = PROJECT_NAME + '/crawled.txt'\n",
    "NUMBER_OF_THREADS = 8\n",
    "queue = Queue()\n",
    "Spider(PROJECT_NAME, base_url, DOMAIN_NAME)\n",
    "\n",
    "# Create worker threads (will die when main exits)\n",
    "def create_workers():\n",
    "    for _ in range(NUMBER_OF_THREADS):\n",
    "        t = threading.Thread(target=work)\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "\n",
    "\n",
    "# Do the next job in the queue\n",
    "def work():\n",
    "    while True:\n",
    "        url = queue.get()\n",
    "        Spider.crawl_page(threading.current_thread().name, url)\n",
    "        queue.task_done()\n",
    "\n",
    "\n",
    "# Each queued link is a new job\n",
    "def create_jobs():\n",
    "    for link in file_to_set(QUEUE_FILE):\n",
    "        queue.put(link)\n",
    "    queue.join()\n",
    "    crawl()\n",
    "\n",
    "\n",
    "# Check if there are items in the queue, if so crawl them\n",
    "def crawl():\n",
    "    queued_links = file_to_set(QUEUE_FILE)\n",
    "    if len(queued_links) > 0:\n",
    "        print(str(len(queued_links)) + ' links in the queue')\n",
    "        create_jobs()\n",
    "\n",
    "\n",
    "create_workers()\n",
    "crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
